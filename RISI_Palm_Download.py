import os
import time
import pandas as pd
import io
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
import pygsheets
from collections import Counter

# --- Configuration ---
RISI_USERNAME = os.getenv('RISI_USERNAME')
RISI_PASSWORD = os.getenv('RISI_PASSWORD')
SERVICE_ACCOUNT_FILE = 'service_account_key.json'
GSHEET_ID = '1Qonj5yKwHVrxApUi7_N2CJtxj61rPfULXALrY4f8lPE'
GSHEET_TITLE = 'Sheet12' # The error log showed 'Sheet11'
CHROMEDRIVER_PATH = os.getenv('CHROMEDRIVER_PATH', '/usr/bin/chromedriver')
DOWNLOAD_DIR = "/tmp/downloads" # For error screenshots

def scrape_table_data(link):
    """
    æŠ“å–åŸå§‹è¡¨æ ¼æ•°æ®ï¼Œæ­¤å‡½æ•°ä¿æŒä¸å˜ã€‚
    """
    options = Options()
    options.binary_location = '/usr/bin/chromium-browser'
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("--start-maximized")

    service = webdriver.chrome.service.Service(executable_path=CHROMEDRIVER_PATH)
    driver = webdriver.Chrome(service=service, options=options)

    try:
        driver.get(link)
        wait = WebDriverWait(driver, 60)

        print("Waiting for login page...")
        wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '#userEmail'))).send_keys(RISI_USERNAME)
        wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '#password'))).send_keys(RISI_PASSWORD)
        wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '#login-button'))).click()
        print("Login successful.")

        print("Waiting for the data grid to load...")
        grid_selector = (By.CSS_SELECTOR, 'div[role="treegrid"]')
        grid_container = wait.until(EC.visibility_of_element_located(grid_selector))
        print("AG-Grid container found. Extracting all raw cell data...")

        fixed_headers = [
            'Month', 'Crude Palm Oil Malaysia', 'RBD Palm Stearin MY',
            'RBD Palm Kernel MY', 'Coconut Oil', 'Crude CNO', 'Tallow',
            'Soybean Oil 1st', 'Soybean Oil 2nd', 'Soybean Oil 3rd'
        ]
        num_expected_columns = len(fixed_headers)
        
        # è¿™é‡Œçš„é€»è¾‘ä¿æŒä¸å˜ï¼Œå®ƒä¼šæŠ“å–åˆ°åˆ†ç¦»çš„è¡Œ
        all_rows_elements = grid_container.find_elements(By.CSS_SELECTOR, '[role="row"]')
        data_rows = []
        for r in all_rows_elements:
            cells = r.find_elements(By.CSS_SELECTOR, '[role="gridcell"]')
            # è¿‡æ»¤æ‰ç©ºçš„headerè¡Œæˆ–ä¸å®Œæ•´çš„è¡Œ
            if cells and len(cells) <= num_expected_columns:
                data_rows.append([c.text for c in cells])

        if not data_rows:
            raise ValueError("Scraping failed: No raw data rows were found.")
        
        # åˆ›å»ºåŸå§‹çš„ã€æœªå¤„ç†çš„DataFrame
        # æ³¨æ„ï¼šè¿™é‡Œçš„åˆ—ååªæ˜¯ä¸€ä¸ªä¸´æ—¶çš„å ä½ç¬¦ï¼Œå› ä¸ºæ•°æ®æ˜¯é”™ä½çš„
        temp_cols = [f'col_{i}' for i in range(num_expected_columns)]
        raw_df = pd.DataFrame(data_rows, columns=temp_cols[:len(data_rows[0])])
        
        print("Successfully created raw DataFrame. It will be processed next.")
        print(raw_df.head())
        
        return raw_df

    except Exception as e:
        print(f"An error occurred during scraping: {e}")
        error_screenshot_path = os.path.join(DOWNLOAD_DIR, 'error_screenshot.png')
        if not os.path.exists(DOWNLOAD_DIR):
            os.makedirs(DOWNLOAD_DIR)
        driver.save_screenshot(error_screenshot_path)
        print(f"Error screenshot saved to: {error_screenshot_path}")
        raise
    finally:
        print("Closing the browser.")
        driver.quit()

def process_and_clean_data(raw_df):
    """
    ğŸ”§ æ–°å¢å‡½æ•°ï¼šæ¸…ç†å’Œé‡ç»„DataFrameã€‚
    å°†æ—¥æœŸè¡Œå’Œæ•°æ®è¡Œåˆå¹¶ï¼Œå¹¶æ ¼å¼åŒ–æ—¥æœŸã€‚
    """
    print("Processing and cleaning the raw data...")
    if raw_df is None or raw_df.empty:
        print("Raw DataFrame is empty, skipping processing.")
        return pd.DataFrame()

    # 1. è®¡ç®—åˆ†å‰²ç‚¹ï¼ˆæ€»è¡Œæ•°çš„ä¸€åŠï¼‰
    num_rows = len(raw_df)
    if num_rows % 2 != 0:
        print(f"Warning: The number of rows ({num_rows}) is odd. Data might be incomplete.")
        return pd.DataFrame() # è¿”å›ç©ºè¡¨ä»¥é¿å…åç»­é”™è¯¯
        
    half_point = num_rows // 2
    
    # 2. æå–æ—¥æœŸéƒ¨åˆ†å’Œæ•°æ®éƒ¨åˆ†
    # æ—¥æœŸåœ¨å‰åŠéƒ¨åˆ†çš„ç¬¬1åˆ—
    dates = raw_df.iloc[:half_point, 0].reset_index(drop=True)
    # æ•°æ®åœ¨ååŠéƒ¨åˆ†ï¼Œä»ç¬¬1åˆ—å¼€å§‹çš„æ‰€æœ‰åˆ—
    # å› ä¸ºåŸç½‘ç«™ç»“æ„é—®é¢˜ï¼Œæ•°æ®è¡Œçš„ç¬¬ä¸€åˆ—æ˜¯ç©ºçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»ç¬¬äºŒåˆ—å¼€å§‹å–
    numeric_data = raw_df.iloc[half_point:].reset_index(drop=True)
    
    # 3. å°†æ—¥æœŸå’Œæ•°æ®æ°´å¹³åˆå¹¶æˆä¸€ä¸ªæ–°çš„DataFrame
    clean_df = pd.concat([dates, numeric_data], axis=1)

    # 4. è®¾ç½®æ­£ç¡®çš„åˆ—å
    clean_df.columns = [
        'Month', 'Crude Palm Oil Malaysia', 'RBD Palm Stearin MY',
        'RBD Palm Kernel MY', 'Coconut Oil', 'Crude CNO', 'Tallow',
        'Soybean Oil 1st', 'Soybean Oil 2nd', 'Soybean Oil 3rd'
    ]

    # 5. è½¬æ¢æ—¥æœŸæ ¼å¼ä» "25 Jul 2025" åˆ° "2025-07-25"
    # ä½¿ç”¨ errors='coerce' ä¼šå°†ä»»ä½•æ— æ³•è½¬æ¢çš„æ—¥æœŸå˜ä¸ºNaT(Not a Time)ï¼Œé¿å…ç¨‹åºä¸­æ–­
    clean_df['Month'] = pd.to_datetime(clean_df['Month'], format='%d %b %Y', errors='coerce').dt.strftime('%Y-%m-%d')

    # åˆ é™¤ä»»ä½•å› ä¸ºæ—¥æœŸè½¬æ¢å¤±è´¥è€Œäº§ç”Ÿçš„ç©ºè¡Œ
    clean_df.dropna(subset=['Month'], inplace=True)
    
    print("Data processing complete. Final DataFrame is ready:")
    print(clean_df.head())
    
    return clean_df

def append_to_gsheet(dataframe, gsheet_id, sheet_title):
    """
    å°†DataFrameé™„åŠ åˆ°Google Sheetï¼Œæ­¤å‡½æ•°ä¿æŒä¸å˜ã€‚
    """
    if dataframe is None or dataframe.empty:
        print("DataFrame is empty. Skipping Google Sheet update.")
        return
    try:
        print(f"Connecting to Google Sheet '{sheet_title}' to append data...")
        gc = pygsheets.authorize(service_file=SERVICE_ACCOUNT_FILE)
        sh = gc.open_by_key(gsheet_id)
        wks = sh.worksheet_by_title(sheet_title)
        
        all_values = wks.get_all_values(include_tailing_empty_rows=False, include_tailing_empty=False)
        last_data_row = len(all_values)
        
        num_new_rows = len(dataframe)
        if (last_data_row + num_new_rows) > wks.rows:
            rows_to_add = (last_data_row + num_new_rows) - wks.rows + 500
            print(f"Sheet is full. Adding {rows_to_add} more rows...")
            wks.add_rows(rows_to_add)
            print("Successfully added more rows.")

        next_empty_row = last_data_row + 1
        print(f"Appending new data starting at row {next_empty_row}...")
        wks.set_dataframe(dataframe, start=(next_empty_row, 1), copy_head=False, nan='')
        
        print(f"Successfully appended data to Google Sheet '{sheet_title}'.")

    except Exception as e:
        print(f"An error occurred during Google Sheet sync: {e}")
        raise

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("Automation task started...")
    
    # æ­¥éª¤ 1: æŠ“å–åŸå§‹çš„ã€æœªå¤„ç†çš„æ•°æ®
    raw_dataframe = scrape_table_data('https://dashboard.fastmarkets.com/sw/x2TtMTTianBBefSdGCeZXc/palm-oil-global-prices')
    
    # æ­¥éª¤ 2: æ¸…ç†å’Œé‡ç»„æ•°æ®
    clean_dataframe = process_and_clean_data(raw_dataframe)
    
    # æ­¥éª¤ 3: å°†å¹²å‡€çš„æ•°æ®ä¸Šä¼ åˆ° Google Sheet
    append_to_gsheet(
        dataframe=clean_dataframe,
        gsheet_id=GSHEET_ID,
        sheet_title=GSHEET_TITLE
    )
    print("Automation task completed successfully! âœ…")

if __name__ == "__main__":
    main()
